{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Opinion Mining Lab - Exercises 2.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z0aegAFNihDz"},"source":["# Opinion Mining & Sentiment Analysis: Exercises (part 2)\n","\n","**Text Mining unit**\n","\n","_Prof. Gianluca Moro, Dott. Ing. Nicola Piscaglia – DISI, University of Bologna_\n","\n","**Bologna Business School** - Alma Mater Studiorum Università di Bologna"]},{"cell_type":"markdown","metadata":{"id":"C9sHk878uM5U"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"wlq0hdJOuWtn"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"MVjtrtsuD3te","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624356834267,"user_tz":-120,"elapsed":1122,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"3bfe97fa-c2fa-4f8c-cd8d-a5028906324b"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","from statsmodels.stats.contingency_tables import mcnemar\n","import os\n","from urllib.request import urlretrieve\n","import glob\n","import gzip\n","import json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oG8M1yTUX65e"},"source":["### Utility functions"]},{"cell_type":"code","metadata":{"id":"7JL5tJHlX4jr"},"source":["# Download a file from an URL\n","def download(file, url):\n","    if not os.path.isfile(file):\n","        urlretrieve(url, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbTxjuJBi6T8"},"source":["def scan_hu_liu(f):\n","    for line in f:\n","        line = line.decode(errors=\"ignore\").strip()\n","        if line and not line.startswith(\";\"):\n","            yield line\n","\n","def load_hu_liu(filename):\n","    with open(filename, \"rb\") as f:\n","        return set(scan_hu_liu(f))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRnNzelIMreh"},"source":["total_samples = 50000\n","\n","def load_dataset(dataset_path):\n","    # We do not consider 3-stars rating as they can be confuse the model in\n","    # opinion polarity discrimination\n","    pos_overall_values = {5.0, 4.0}\n","    neg_overall_values = {1.0, 2.0}\n","    \n","    # Data structures\n","    data = []\n","    overall = []\n","\n","    # Data loading: read all the dataset\n","    print(\"Loading json file...\")\n","    \n","    # Reading dataset: we build our dataset by selecting\n","    # only the reviews which are not 3 stars rated. For each review added to our\n","    # dataset we add to \"overall\" the label for that review.\n","    with gzip.open(dataset_path) as jsonfile:\n","        index = 0\n","        pos = 0\n","        neg = 0\n","        # Each line in the json file represents a review\n","        for line in jsonfile:\n","            review = json.loads(line)\n","            # \n","            if review['overall'] in pos_overall_values and pos < int(total_samples / 2):\n","                index += 1\n","                # We keep track of the number of positive reviews read\n","                pos += 1\n","                # Review appending to our dataset\n","                data.append(review)\n","\n","                # Label appending\n","                overall.append(1)\n","                if index >= total_samples:\n","                    break\n","            elif review['overall'] in neg_overall_values and neg < int(total_samples / 2):\n","                index += 1\n","                # We keep track of the number of negative reviews read\n","                neg += 1\n","                data.append(review)\n","                # Label appending\n","                overall.append(0)\n","                # We stop reading if we reached the maximum number of samples\n","                if index >= total_samples:\n","                    break\n","\n","    df = pd.DataFrame.from_dict(data)[[\"reviewText\", \"overall\"]]\n","    df[\"reviewText\"] = df[\"reviewText\"].apply(lambda x: np.str_(x)) # encoding the strings as unicode ones\n","    df[\"overall\"] = np.where(df[\"overall\"] >= 4, 1, 0)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brPNp3uSldQC"},"source":["def mcnemar_pval(model1_predictions, model2_predictions):\n","    # define contingency table\n","    table = pd.crosstab(model1_predictions, model2_predictions)\n","    print(table)\n","\n","    # calculate mcnemar test\n","    result = mcnemar(table)\n","    return result.pvalue"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wnkMJMkxYA7F"},"source":["### Datasets Downloading"]},{"cell_type":"code","metadata":{"id":"IjkqeJGrrdH_"},"source":["# Dataset downloading\n","download('AMAZON_FASHION.json.gz','http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/AMAZON_FASHION.json.gz')\n","download(\"Software.json.gz\", \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Software.json.gz\")\n","download(\"positive-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/3ad6643b698f652f200dfbf463a3cb49de8c0e9f/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/positive-words.txt\")\n","download(\"negative-words.txt\", \"https://raw.githubusercontent.com/unibodatascience/BBS-TextMining/3ad6643b698f652f200dfbf463a3cb49de8c0e9f/05%20-%20Opinion%20Mining%20with%20Python%20(part%201)/data/negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YAjmHi5YHrG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624356834270,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"925bbcef-3fcf-4a21-ddf4-c8f297e66fee"},"source":["# Check if the files have been successfully downloaded\n","print(glob.glob(\"*\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Software.json.gz', 'Gift_Cards.json.gz', 'positive-words.txt', 'negative-words.txt', 'sample_data']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wvsmOXnZY82_"},"source":["### Datasets loading"]},{"cell_type":"code","metadata":{"id":"4LrnAxV1P1c2"},"source":["pos_words = load_hu_liu(\"positive-words.txt\")\n","neg_words = load_hu_liu(\"negative-words.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"jQHe-jEjzfgp","executionInfo":{"status":"ok","timestamp":1624358760599,"user_tz":-120,"elapsed":3385,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"8418ea39-63cd-4dc0-c610-82a03a484c69"},"source":["# Loading data into a pandas dataframe\n","reviews = load_dataset('AMAZON_FASHION.json.gz')\n","reviews.head() # print first 5 entries"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading json file...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviewText</th>\n","      <th>overall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Exactly what I needed.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I agree with the other review, the opening is ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Love these... I am going to order another pack...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>too tiny an opening</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Exactly what I wanted.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          reviewText  overall\n","0                             Exactly what I needed.        1\n","1  I agree with the other review, the opening is ...        0\n","2  Love these... I am going to order another pack...        1\n","3                                too tiny an opening        0\n","4                             Exactly what I wanted.        1"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"KkWec0HHPztM","executionInfo":{"status":"ok","timestamp":1624358762731,"user_tz":-120,"elapsed":2140,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"505fa883-f8a9-4d5e-bf6b-d776ecf90e92"},"source":["reviews_B = load_dataset('Software.json.gz')\n","reviews_B.head() # print first 5 entries"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading json file...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviewText</th>\n","      <th>overall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The materials arrived early and were in excell...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I am really enjoying this book with the worksh...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I have used LearnSmart and can officially say ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Strong backgroung, good read, quite up to date...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          reviewText  overall\n","0  The materials arrived early and were in excell...        1\n","1  I am really enjoying this book with the worksh...        1\n","2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...        0\n","3  I have used LearnSmart and can officially say ...        1\n","4  Strong backgroung, good read, quite up to date...        1"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"RvZ0WZAyywQu"},"source":["### Exercises"]},{"cell_type":"markdown","metadata":{"id":"GJADgDZFihEG"},"source":["1) Check the two loaded datasets cardinality and their distribution by label (`overall` field)"]},{"cell_type":"markdown","metadata":{"id":"RZUDfp1kC3Ea"},"source":["2) Then, randomly split the loaded dataset into training and test set\n","* using an hold-out approach (e.g. the training set is composed by the 70% of the dataset reviews)\n","* keeping the splits cardinality balanced by label"]},{"cell_type":"markdown","metadata":{"id":"c357B6OmihEY"},"source":["3)  A lexicon with sets of commonly used positive and negative words is provided in the variables pos_words and neg_words, respectively.\n","\n","Setup NLTK and define the scoring function to classify all the reviews by first assigning to each a score equal to the number of known positive words within it minus the number of negative words, then return 1 for reviews with a positive score and 0 for reviews with a negative or null score."]},{"cell_type":"markdown","metadata":{"id":"za-2hKEAihEi"},"source":["4) Apply the function to all reviews contained in the test set of `review`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HQmMUoxBihEo"},"source":["5) Compare the obtained labels with the known ones and compute the accuracy as the ratio of matches"]},{"cell_type":"markdown","metadata":{"id":"NyroDWVoihEz"},"source":["6) Create a pipeline including a `CountVectorizer` to convert reviews into word count vectors and a `LogisticRegression` model"]},{"cell_type":"markdown","metadata":{"id":"9qy6cAiuihE5"},"source":["7) Train the model on the training set of the `review` dataset "]},{"cell_type":"markdown","metadata":{"id":"RdC3yKxxihFC"},"source":["8) Evaluate the model on the test set of the `review` dataset "]},{"cell_type":"markdown","metadata":{"id":"Crs7chcBihFP"},"source":["9) Repeat steps from 6 to 8, but replacing the `CountVectorizer` in the pipeline with a `TfidfVectorizer`"]},{"cell_type":"markdown","metadata":{"id":"1SCmzFpYihFm"},"source":["10) Repeat steps from 6 to 8, as above, but set the `ngram_range` parameter of the `TfidfVectorizer` to include bigrams"]},{"cell_type":"markdown","metadata":{"id":"-Z8-7qwTihF4"},"source":["11) Repeat the evaluation phase of the three models above, this time on all the reviews from `reviews_B` dataset"]},{"cell_type":"markdown","metadata":{"id":"WfWZY_hUihGI"},"source":["12) Extract the predictions from `reviews` test set for each classifier: we already have those for the unsupervised one (`preds_1`), we need those from the supervised models"]},{"cell_type":"markdown","metadata":{"id":"p-IJ0MF3ihGM"},"source":["13) Perform comparisons between every pair of models using the provided `mcnemar_pval(p1, p2)` function"]},{"cell_type":"markdown","metadata":{"id":"XkEokwmktSlW"},"source":["Set a confidence level to consider for rejecting the null hypothesis (H0) e.g. 95%.\n","\n","H0 states the two classifiers present the same proportions of errors (they disagree to the same amount)."]},{"cell_type":"markdown","metadata":{"id":"AOX6S2ApihGk"},"source":["**In each case, the closer the p-value to 0, the more compared classifiers are different**"]}]}