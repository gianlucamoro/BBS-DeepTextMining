{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Opinion_Mining_&_Sentiment_Analysis_Deep_Learning_Techniques_Bologna_Business_School.ipynb","provenance":[],"collapsed_sections":["phOqAYXkijh9"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X5-GCnvKijfl"},"source":["# Opinion Mining & Sentiment Analysis: Deep Learning Techniques\n","\n","**Text Mining unit**\n","\n","_Prof. Gianluca Moro^, Dott. Ing. Nicola Piscaglia° – DISI, University of Bologna_\n","\n","^name.surname@unibo.it\n","\n","\n","°name.surname@bbs.unibo.it\n","\n","\n","**Bologna Business School** - Alma Mater Studiorum Università di Bologna"]},{"cell_type":"markdown","metadata":{"id":"hdU4Z5sWijfn"},"source":["## Setup\n","\n","Import external libraries (thus verifying they are correctly installed)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luAnTvuXijfp","executionInfo":{"status":"ok","timestamp":1623916767348,"user_tz":-120,"elapsed":5839,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"fd488aad-d5be-417f-988b-7583aa3aeffe"},"source":["%tensorflow_version 1.x\n","\n","import gzip\n","import numpy as np\n","import pandas as pd\n","import gensim\n","import tensorflow as tf\n","import keras\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bbc8oQnRun1S"},"source":["Check GPU and limit memory usage"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jGBfE49k62l","executionInfo":{"status":"ok","timestamp":1623916768984,"user_tz":-120,"elapsed":1642,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"b3d0058b-735c-4436-85c0-79db06dbacc7"},"source":["devices = tf.config.experimental_list_devices()\n","\n","[print(device) for device in devices] # print all devices\n","\n","#!nvidia-smi # check GPU configuration\n","\n","if devices:\n","  try:\n","    tf.config.experimental.set_memory_growth = True\n","    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","    gpus = tf.config.experimental.list_physical_devices('GPU')\n","    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","  except RuntimeError as e:\n","    # Virtual devices must be set before GPUs have been initialized\n","    print(e)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/job:localhost/replica:0/task:0/device:CPU:0\n","/job:localhost/replica:0/task:0/device:XLA_CPU:0\n","/job:localhost/replica:0/task:0/device:XLA_GPU:0\n","/job:localhost/replica:0/task:0/device:GPU:0\n","1 Physical GPUs, 1 Logical GPUs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vRGJ_Ufdijf0"},"source":["Define a utility function to download data files if they are not already present in working directory"]},{"cell_type":"code","metadata":{"id":"tMRsk6PDijf1","executionInfo":{"status":"ok","timestamp":1623916775848,"user_tz":-120,"elapsed":255,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["import os\n","from urllib.request import urlretrieve\n","def download(file, url):\n","    if not os.path.isfile(file):\n","        urlretrieve(url, file)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoSfmx3cijf7"},"source":["## Deep Learning and Neural Networks for Text Mining and Sentiment Analysis\n","\n","_Deep learning_ denotes a general approach to machine learning employing **multi-layered models** to obtain **accurate representation of input data**: features are no longer extracted manually, but infered in the learning process\n","\n","DL is mostly based on _neural networks_, very flexible learning models with arbitrary complexity\n","\n","The use of DL and NN allows deep understanding of text without manually defining rules, lexicons, etc."]},{"cell_type":"markdown","metadata":{"id":"22jD9LW4ijf9"},"source":["## TensorFlow and Keras\n","\n","- **TensorFlow** by Google is one of the most used computation frameworks for deep learning\n","  - TF works by building a _computational graph_ where each node represents an operation between _tensors_ (N-dimensional arrays)\n","    - sums, products, derivatives, ...\n","  - a graph can run either on CPU or (where available) on GPU for accelerated parallel computation\n","- **Keras** provides an high-level API for building and training neural networks using TensorFlow as a backend\n","  - networks can be built simply by stacking different layers with many configurable (hyper)parameters\n","  - high-level commands are provided to train and evaluate networks on given datasets"]},{"cell_type":"markdown","metadata":{"id":"4MldDiptijf-"},"source":["## Dataset: Movie Reviews\n","\n","We have a collection of user reviews extracted from IMDb (the _Internet Movie Database_) labeled as positive or negative\n","\n","We want to train a model to understand the positive or negative orientation of any review\n","\n","We start by loading the training dataset, containing 25,000 samples with two attributes\n","- `label` indicates the orientation of the review, can be \"pos\" or \"neg\"\n","- `text` contains the full text of the review"]},{"cell_type":"code","metadata":{"id":"dJZSpzfEijf_","executionInfo":{"status":"ok","timestamp":1623916794596,"user_tz":-120,"elapsed":1663,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"imdb-train.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-train.csv.gz\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixJ7gGxvijgD","executionInfo":{"status":"ok","timestamp":1623916873882,"user_tz":-120,"elapsed":717,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["train_set = pd.read_csv(\"imdb-train.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u20J-GJNijgH","executionInfo":{"status":"ok","timestamp":1623916874265,"user_tz":-120,"elapsed":7,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"18cb5c89-0fdd-48b6-bd6b-d56c0c869abb"},"source":["train_set.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 2)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"GX30ypnMijgM"},"source":["Let's view some rows of the dataset, after increasing the lenght of shown text"]},{"cell_type":"code","metadata":{"id":"W7zi6nMDijgN","executionInfo":{"status":"ok","timestamp":1623916877171,"user_tz":-120,"elapsed":5,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["pd.options.display.max_colwidth = 100"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"s7tSOPO7ijgR","executionInfo":{"status":"ok","timestamp":1623916888139,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"678c4736-ab38-4154-fad3-f1d1b23c0f4e"},"source":["train_set.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neg</td>\n","      <td>Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a t...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>If you like adult comedy cartoons, like South Park, then this is nearly a similar format about t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neg</td>\n","      <td>Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so w...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this sea...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                                                                                 text\n","0   pos  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...\n","1   neg  Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a t...\n","2   pos  If you like adult comedy cartoons, like South Park, then this is nearly a similar format about t...\n","3   neg  Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so w...\n","4   pos  Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this sea..."]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"zBDgzSABijgX"},"source":["Some HTML tags are present within reviews, specifically `<br />` to indicate newlines: we write a function which, applied on a text, replaces such tags with ASCII newline `\\n`"]},{"cell_type":"code","metadata":{"id":"hfNSsvFDijgY","executionInfo":{"status":"ok","timestamp":1623916900848,"user_tz":-120,"elapsed":301,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["def strip_tags(text):\n","    return text.replace(\"<br />\", \"\\n\")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"su4ya93jijgd"},"source":["We apply the function to all texts in the dataset"]},{"cell_type":"code","metadata":{"id":"P_QwwWL5ijge","executionInfo":{"status":"ok","timestamp":1623916902689,"user_tz":-120,"elapsed":274,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["train_set[\"text\"] = train_set[\"text\"].apply(strip_tags)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBKiXF1Tijgh"},"source":["Positive and negative reviews are evenly distributed"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNlJoLAjijgi","executionInfo":{"status":"ok","timestamp":1623916904820,"user_tz":-120,"elapsed":255,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"2268e655-3af7-474b-9344-a017c61c49bd"},"source":["train_set[\"label\"].value_counts()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    12500\n","pos    12500\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ckkhL-emijgl"},"source":["## Multi-Layer Perceptron\n","\n","In their usual form, neural networks are composed by a stack of _densely connected_ layers of nodes: each node in a layer receives the output of all nodes of the underlying layer. Such networks are also known as _multi-layer perceptrons_.\n","\n","A MLP receives a vector as input and its topmost layer produces a vector as output, an arbitrary number of _hidden layers_ can be inserted inbetween to produce intermediate representations of data"]},{"cell_type":"markdown","metadata":{"id":"3nmAT3olijgm"},"source":["Let's start by training a neural network for sentiment classification feeded with vector space model representations of reviews\n","\n","We initialize a vector space using tf.idf term weighting and filtering out very rare terms"]},{"cell_type":"code","metadata":{"id":"kXHi0YZoijgm","executionInfo":{"status":"ok","timestamp":1623916908129,"user_tz":-120,"elapsed":246,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vect = TfidfVectorizer(min_df=3)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8DNh4VIijgp"},"source":["Such vector space is built upon the training reviews and their document-term matrix is produced"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WQsBrJBijgq","executionInfo":{"status":"ok","timestamp":1623916913653,"user_tz":-120,"elapsed":3671,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"27f18d55-6ee3-42fa-a761-35f7939742a4"},"source":["train_dtm = vect.fit_transform(train_set[\"text\"])\n","train_dtm"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<25000x35852 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 3383253 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"vBbLIk4sijgt"},"source":["Similarly to training matrices for scikit-learn models, this is a 2D array where each row (1st axis) is a training observation and each column (2nd axis) is a feature: each row is a possible input to the neural network"]},{"cell_type":"markdown","metadata":{"id":"1nIpGSnPijgu"},"source":["We extract the number of distinct terms in the vector space, used to define the structure of the neural network"]},{"cell_type":"code","metadata":{"id":"75ljR5t8ijgu","executionInfo":{"status":"ok","timestamp":1623916916897,"user_tz":-120,"elapsed":273,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["num_terms = len(vect.get_feature_names())"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTpOW3u_ijgx","executionInfo":{"status":"ok","timestamp":1623916918803,"user_tz":-120,"elapsed":275,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"0fbb51a2-2054-4b43-9181-56bad76362d1"},"source":["num_terms"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35852"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"2BPRJINJijgz"},"source":["We want our neural network to indicate in output the correct class of each review, either \"pos\" or \"neg\"\n","\n","The common approach to classification with neural networks is to have one output node for each class and train them to output 1 on the right node and 0 on the others\n","\n","For this, we must extract from the `label` column a \"target\" matrix, where each row contains the values which the network should give as output for each review\n","- `[1, 0]` for positive reviews\n","- `[0, 1]` for negative reviews\n","\n","We define a function `make_target` which converts a given pos/neg labels series into a target matrix"]},{"cell_type":"code","metadata":{"id":"4Ag3JHLcijgz","executionInfo":{"status":"ok","timestamp":1623916929308,"user_tz":-120,"elapsed":281,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["def make_target(labels):\n","    return pd.DataFrame({\n","        \"pos\": labels == \"pos\", # if the label is \"pos\" then return 1 else return 0\n","        \"neg\": labels == \"neg\" # if the label is \"neg\" then return 1 else 0\n","    }).astype(int)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5u1aRO_ijg3"},"source":["We then apply it to the training set labels"]},{"cell_type":"code","metadata":{"id":"-ri-b_2sijg4","executionInfo":{"status":"ok","timestamp":1623916931462,"user_tz":-120,"elapsed":270,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["train_target = make_target(train_set[\"label\"])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XdFtJC0eijg7"},"source":["We obtain a matrix where each row is the expected network output, either `[1, 0]` (positive) or `[0, 1]` (negative)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"-ZnzNjEQijg8","executionInfo":{"status":"ok","timestamp":1623916937737,"user_tz":-120,"elapsed":408,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"906c0a3e-f0b9-4ad3-e8c2-99ed3fa92f51"},"source":["train_target.head()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>neg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pos  neg\n","0    1    0\n","1    0    1\n","2    1    0\n","3    0    1\n","4    1    0"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"ZnexXb3qijg_"},"source":["Let's define the structure of the neural network\n","\n","We create a _sequential_ model, i.e. we define the network as a sequence of interconnected layers (alternatively, we could create non-linear structures by manually connecting layers to each other using Keras Functional API. To learn more, see: https://keras.io/guides/functional_api/)"]},{"cell_type":"code","metadata":{"id":"nNu5VDT4ijhA","executionInfo":{"status":"ok","timestamp":1623916941502,"user_tz":-120,"elapsed":268,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from keras.models import Sequential\n","model = Sequential()"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiFVkuuwyT0-"},"source":["#Functional API version\n","# inputs = keras.Input(shape=(num_terms,))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"syikncPfijhC"},"source":["In this first example we create a single-layered network, where inputs are directly connected to the output nodes\n","\n","As discussed above, the output nodes must be 2, one for each class; we use the _softmax_ activation function to ensure that the output is a valid probability distribution\n","- we will never get a perfect `[1, 0]` as output in practice, but we will get outputs like `[0.99, 0.01]`\n","\n","In the first layer (in this case the only one) we also have to specify with `input_dim` the size of input vectors, in this case the number of terms in the vector space"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K387d-7uijhD","executionInfo":{"status":"ok","timestamp":1623916960874,"user_tz":-120,"elapsed":240,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"bb87dfc2-ba71-45d2-8543-9510ab7fad80"},"source":["from keras.layers import Dense\n","model.add(Dense(2, activation=\"softmax\", input_dim=num_terms))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MprGhojHyFu8"},"source":["# Functional API version\n","# outputs = keras.layers.Dense(2, activation=\"softmax\", input_dim=num_terms)(inputs)\n","# model = keras.Model(inputs=inputs, outputs=outputs, name=\"my_model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uiVFxjStijhF"},"source":["With `summary` we can analyze the structure of our network and get the count of trainable parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaNQ0pm-ijhF","executionInfo":{"status":"ok","timestamp":1623916965088,"user_tz":-120,"elapsed":245,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"f9924484-1eb7-439b-bff8-b2910bbc21eb"},"source":["model.summary()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 2)                 71706     \n","=================================================================\n","Total params: 71,706\n","Trainable params: 71,706\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8J83C6k9ijhI"},"source":["In this case we have 35,852×2 = 71,704 weights + 2 biases = 71,706 trainable parameters"]},{"cell_type":"markdown","metadata":{"id":"iHMrkxAbijhI"},"source":["After defining the network structure, we _compile_ it to provide some general settings of the network and initialize accordingly the underlying TensorFlow data structures"]},{"cell_type":"code","metadata":{"id":"rh-BWg-bijhJ","executionInfo":{"status":"ok","timestamp":1623916987620,"user_tz":-120,"elapsed":292,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nr3kQaO6ijhM"},"source":["- The _optimizer_ is the algorithm used to train the network: _Adam_ and other valid options are different variants of stochastic gradient descent (SGD), including learning rate decay, momentum, etc.\n","- The _loss_ is the measure to be minimized in the training process: the _cross entropy_ penalizes outputs which are not close to 1 on the correct class\n","- Additional _metrics_ can be computed for evaluation purposes: we use the accuracy, i.e. the percentage of correctly classified examples"]},{"cell_type":"markdown","metadata":{"id":"KC5OK7bhijhM"},"source":["We are now ready to train (_fit_) the network on given training examples, composed of inputs (tf.idf vectors) and target outputs (`[1, 0]` for positive reviews and `[0, 1]` for negative)\n","\n","The training examples are shuffled and used to run SGD steps on _minibatches_ of a specified size (`batch_size`): this process is repeated for a given number of training _epochs_\n","\n","`callbacks` parameters let us define a list of functions to be called by keras at the end of each epoch. This option is mainly used to implement some training logic (e.g. EarlyStopping) but if we runned out of memory we also could use it to call gc.collect() after each epoch in order to free some memory."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoX-MmG1ijhN","executionInfo":{"status":"ok","timestamp":1623917039429,"user_tz":-120,"elapsed":49318,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"e1e08b7b-2817-4f9a-d6ad-7eb1c8b97677"},"source":["# Garbage Collector library\n","import gc\n","\n","# Custom Callback To Include in Callbacks List At Training Time\n","class GarbageCollectorCallback(keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        gc.collect()\n","\n","fit_history = model.fit(train_dtm, train_target, batch_size=200, epochs=10, callbacks=[GarbageCollectorCallback()])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Epoch 1/10\n","25000/25000 [==============================] - 6s 225us/step - loss: 0.6566 - accuracy: 0.8114\n","Epoch 2/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.5869 - accuracy: 0.8650\n","Epoch 3/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.5326 - accuracy: 0.8765\n","Epoch 4/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.4890 - accuracy: 0.8858\n","Epoch 5/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.4534 - accuracy: 0.8938\n","Epoch 6/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.4235 - accuracy: 0.8994\n","Epoch 7/10\n","25000/25000 [==============================] - 4s 155us/step - loss: 0.3981 - accuracy: 0.9062\n","Epoch 8/10\n","25000/25000 [==============================] - 4s 155us/step - loss: 0.3762 - accuracy: 0.9108\n","Epoch 9/10\n","25000/25000 [==============================] - 4s 156us/step - loss: 0.3571 - accuracy: 0.9158\n","Epoch 10/10\n","25000/25000 [==============================] - 4s 155us/step - loss: 0.3401 - accuracy: 0.9198\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G_rHoQ3GijhO"},"source":["During the training process we see how loss and accuracy measured on the training set vary, their evolution can also be obtained from the \"history\" object returned by `fit`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"5ZAUswkRijhP","executionInfo":{"status":"ok","timestamp":1623917042086,"user_tz":-120,"elapsed":329,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"dbdafa30-9c8f-484f-c8f9-931ecdba3a99"},"source":["plt.plot(fit_history.history[\"loss\"], \"ro-\")\n","plt.plot(fit_history.history[\"accuracy\"], \"bo-\")\n","plt.legend([\"Train loss\", \"Train accuracy\"]);"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"g8IEIw07ijhQ"},"source":["We can see in the plot how the loss progressively decreases and accuracy progressively increases through training epochs"]},{"cell_type":"markdown","metadata":{"id":"RMJIumEVijhQ"},"source":["To get the raw output given by the network for a given input, we use the `predict` method: let's see for example the output for the first training review (labeled positive)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XS1kfh2gUS8l","executionInfo":{"status":"ok","timestamp":1623917046085,"user_tz":-120,"elapsed":243,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"b83785db-be98-45c2-a9b9-d67f38315187"},"source":["train_dtm[0]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<1x35852 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 86 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9xvYkoBijhR","executionInfo":{"status":"ok","timestamp":1623917049199,"user_tz":-120,"elapsed":247,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"3de78a37-eebc-43cd-ef5a-441dde466147"},"source":["model.predict(train_dtm[0])"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.5996168 , 0.40038314]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"Z0mqomIPijhS"},"source":["We see that the first class (positive) has higher probability\n","\n","We can directly get the predicted class index with `predict_classes`"]},{"cell_type":"code","metadata":{"id":"ZHZq9_RWijhS"},"source":["# model.predict_classes(train_dtm[0].toarray()) --> [0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytFtEhPwWABp"},"source":["or..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOjYjx9CU-aI","executionInfo":{"status":"ok","timestamp":1623917053162,"user_tz":-120,"elapsed":258,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"11d28725-ac2f-4bc9-d136-3c76ebb32eac"},"source":["# Alternative Version working with Functional API / Tensorflow 2.0\n","prediction = model.predict(train_dtm[0])\n","print(prediction)\n","\n","np.argmax(prediction, axis=-1)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[[0.5996168  0.40038314]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"g4S_QpdtijhU"},"source":["Let's now evaluate the network on a separate test set of labeled reviews, provided in the `imdb-test.csv.gz` file"]},{"cell_type":"code","metadata":{"id":"mAloDfs_ijhU","executionInfo":{"status":"ok","timestamp":1623917066713,"user_tz":-120,"elapsed":1351,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"imdb-test.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-test.csv.gz\")"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLNJREo7ijhW","executionInfo":{"status":"ok","timestamp":1623917068750,"user_tz":-120,"elapsed":444,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["test_set = pd.read_csv(\"imdb-test.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"cCVKmpdHijhX","executionInfo":{"status":"ok","timestamp":1623917070060,"user_tz":-120,"elapsed":264,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"be7e447a-2806-4dfc-e15c-695e7a723f5b"},"source":["test_set.head(5)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neg</td>\n","      <td>Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved th...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neg</td>\n","      <td>This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>My yardstick for measuring a movie's watch-ability is if I get squirmy. If I start shifting posi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                                                                                 text\n","0   pos  I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...\n","1   neg  Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...\n","2   pos  My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved th...\n","3   neg  This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and...\n","4   pos  My yardstick for measuring a movie's watch-ability is if I get squirmy. If I start shifting posi..."]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"4tewY_AhijhZ"},"source":["Also in this dataset we have 25,000 reviews evenly distributed"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcMI31SWijha","executionInfo":{"status":"ok","timestamp":1623917072667,"user_tz":-120,"elapsed":305,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"5ed3754b-9f8c-435d-8381-331387ff2e05"},"source":["test_set[\"label\"].value_counts()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    12500\n","pos    12500\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"emn3o3LNijhc"},"source":["As before, we apply the HTML strip function to reviews"]},{"cell_type":"code","metadata":{"id":"OxuwoPh7ijhc","executionInfo":{"status":"ok","timestamp":1623917074934,"user_tz":-120,"elapsed":400,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["test_set[\"text\"] = test_set[\"text\"].apply(strip_tags)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-clrrU8lijhf"},"source":["We represent the test reviews in the vector space created on training reviews"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ez5mF5gDijhf","executionInfo":{"status":"ok","timestamp":1623917080394,"user_tz":-120,"elapsed":3699,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"cb0dcd68-d117-4546-fe59-2aa553029d92"},"source":["test_dtm = vect.transform(test_set[\"text\"])\n","test_dtm"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<25000x35852 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 3291409 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"TSX4Xm2_ijhh"},"source":["We then convert pos/neg labels for test examples into target vectors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"JaIS1xc4ijhi","executionInfo":{"status":"ok","timestamp":1623917083344,"user_tz":-120,"elapsed":280,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"60a5941c-9e6b-4d01-e492-f2f90ec60910"},"source":["test_target = make_target(test_set[\"label\"])\n","test_target"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>neg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 2 columns</p>\n","</div>"],"text/plain":["       pos  neg\n","0        1    0\n","1        0    1\n","2        1    0\n","3        0    1\n","4        1    0\n","...    ...  ...\n","24995    0    1\n","24996    1    0\n","24997    0    1\n","24998    1    0\n","24999    0    1\n","\n","[25000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"p3kfF1gxijhj"},"source":["After processing the test set, we can fed it to the neural network for evaluation using the `evaluate` method"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7apXO-kijhk","executionInfo":{"status":"ok","timestamp":1623917089991,"user_tz":-120,"elapsed":4022,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"84479729-1eec-493d-f6d8-236166d92fe2"},"source":["model.evaluate(test_dtm, test_target)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 4s 158us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.39950748119354246, 0.8703600168228149]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"_Gj_OgxTijhm"},"source":["The method reports the loss (first value) and the accuracy (second value) measured on the given test set: our final goal is to maximize the accuracy"]},{"cell_type":"markdown","metadata":{"id":"xbDiUSuVijhm"},"source":["Let's now introduce a _hidden layer_ in the network between input and output, for example a layer of 128 nodes with linear activation which receive input vectors"]},{"cell_type":"code","metadata":{"id":"nyvQ21f2ijhm","executionInfo":{"status":"ok","timestamp":1623917115992,"user_tz":-120,"elapsed":272,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model = Sequential()\n","model.add(Dense(128, input_dim=num_terms))"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZtBdx0p5P-t"},"source":["# Functional API version\n","# inputs = keras.layers.Input(shape=(num_terms,))\n","# x = Dense(128)(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYqtpAIyijho"},"source":["The output of these 128 will be fed to the output layer, composed as above by 2 nodes with softmax activation"]},{"cell_type":"code","metadata":{"id":"CjpSTK-3ijhp","executionInfo":{"status":"ok","timestamp":1623917117399,"user_tz":-120,"elapsed":6,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model.add(Dense(2, activation=\"softmax\"))"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQWstbwR5fH7"},"source":["# Functional API Version\n","# outputs = Dense(2, activation=\"softmax\")(x)\n","# model = keras.Model(inputs = inputs, outputs=outputs, name=\"model_with_hidden_layer\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UH97JkRfijhr"},"source":["The number of network parameters to be trained is now much higher"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77gkoVQOijhr","executionInfo":{"status":"ok","timestamp":1623917119252,"user_tz":-120,"elapsed":368,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"b5a5117e-f387-4f0b-fe10-733d79e0708e"},"source":["model.summary()"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 128)               4589184   \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 258       \n","=================================================================\n","Total params: 4,589,442\n","Trainable params: 4,589,442\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ya32qKjyijht"},"source":["Let's compile the network as before"]},{"cell_type":"code","metadata":{"id":"44A5mJmpijht","executionInfo":{"status":"ok","timestamp":1623917122015,"user_tz":-120,"elapsed":267,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgf2l7fqijhu"},"source":["To keep compute times limited, we fit this and subsequent networks running only 3 training epochs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbNLJJMLijhu","executionInfo":{"status":"ok","timestamp":1623917137228,"user_tz":-120,"elapsed":14159,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"3f756046-8bb6-410b-921d-47e5ce22250d"},"source":["model.fit(train_dtm, train_target, batch_size=200, epochs=3, callbacks=[GarbageCollectorCallback()])"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","25000/25000 [==============================] - 4s 177us/step - loss: 0.4081 - accuracy: 0.8496\n","Epoch 2/3\n","25000/25000 [==============================] - 4s 175us/step - loss: 0.1641 - accuracy: 0.9448\n","Epoch 3/3\n","25000/25000 [==============================] - 4s 175us/step - loss: 0.0878 - accuracy: 0.9767\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa0d92c83d0>"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"t452JASpijhw"},"source":["Let's evaluate this new network on the same test set as before"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7ZIIDp2ijhw","executionInfo":{"status":"ok","timestamp":1623917219590,"user_tz":-120,"elapsed":4450,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"d99a22dc-a87d-4e53-f65f-73ba5b41c77e"},"source":["model.evaluate(test_dtm, test_target)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 4s 168us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.3165155773496628, 0.8718000054359436]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"UxhI79Tuijhy"},"source":["Thanks to the hidden layer we had a very slight improvement, despite the lower number of training epochs"]},{"cell_type":"markdown","metadata":{"id":"LT68VXmSijhy"},"source":["To make the model more expressive, we have to introduce non-linearity in hidden layers: for example, we replicate the model above using sigmoid activation in the hidden layer\n","\n","We can create the model more concisely by providing the list of layers to be stacked"]},{"cell_type":"code","metadata":{"id":"Xn9Rz1nNijhy","executionInfo":{"status":"ok","timestamp":1623917221921,"user_tz":-120,"elapsed":271,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model = Sequential([\n","    Dense(128, activation=\"sigmoid\", input_dim=num_terms),\n","    Dense(2, activation=\"softmax\")\n","])\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4V-pU5mijh0","executionInfo":{"status":"ok","timestamp":1623917238288,"user_tz":-120,"elapsed":14149,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"0d841a47-f768-48ea-d43a-9a09a5b0a633"},"source":["model.fit(train_dtm, train_target, batch_size=200, epochs=3, callbacks=[GarbageCollectorCallback()])"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","25000/25000 [==============================] - 4s 178us/step - loss: 0.6400 - accuracy: 0.7332\n","Epoch 2/3\n","25000/25000 [==============================] - 4s 175us/step - loss: 0.4768 - accuracy: 0.8814\n","Epoch 3/3\n","25000/25000 [==============================] - 4s 175us/step - loss: 0.3393 - accuracy: 0.9079\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa0d91c3e90>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSv2Qj2rijh2","executionInfo":{"status":"ok","timestamp":1623917243798,"user_tz":-120,"elapsed":4335,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"dd048dd8-ce85-4d28-f706-dab9b48ad334"},"source":["model.evaluate(test_dtm, test_target)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 4s 173us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.3578516159439087, 0.8736400008201599]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"81z0mJXgijh3"},"source":["Finally, let's test a deep model with three non-linear hidden layers"]},{"cell_type":"code","metadata":{"id":"WeIVFiroijh3","executionInfo":{"status":"ok","timestamp":1623917255986,"user_tz":-120,"elapsed":270,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model = Sequential([\n","    Dense(256, activation=\"sigmoid\", input_dim=num_terms),\n","    Dense(64, activation=\"sigmoid\"),\n","    Dense(16, activation=\"sigmoid\"),\n","    Dense(2, activation=\"softmax\")\n","])\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4MRD8OIYijh5","executionInfo":{"status":"ok","timestamp":1623917258254,"user_tz":-120,"elapsed":293,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"e214dd5a-d108-429a-a45a-0d0754c93b25"},"source":["model.summary()"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 256)               9178368   \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 64)                16448     \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 16)                1040      \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 2)                 34        \n","=================================================================\n","Total params: 9,195,890\n","Trainable params: 9,195,890\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PolQv6Eijh6","executionInfo":{"status":"ok","timestamp":1623917276471,"user_tz":-120,"elapsed":15549,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"8ab10a75-fac5-4d66-dacb-c8630071f45e"},"source":["model.fit(train_dtm, train_target, batch_size=200, epochs=3)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","25000/25000 [==============================] - 5s 200us/step - loss: 0.6891 - accuracy: 0.5857\n","Epoch 2/3\n","25000/25000 [==============================] - 5s 194us/step - loss: 0.4917 - accuracy: 0.8324\n","Epoch 3/3\n","25000/25000 [==============================] - 5s 195us/step - loss: 0.2129 - accuracy: 0.9230\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa0d5388ad0>"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VU-3khHcijh8","executionInfo":{"status":"ok","timestamp":1623917398860,"user_tz":-120,"elapsed":4454,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"0c4a0c23-fa82-427b-abb2-d509c15bf774"},"source":["model.evaluate(test_dtm, test_target)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 4s 172us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.27668741213798526, 0.8846799731254578]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"phOqAYXkijh9"},"source":["## Word Embedding\n","\n","A _word embedding_ model is a dictionary mapping each known word to a **N-dimensional vector**\n","\n","Such model is built by training a neural network on a bunch of text to predict the most likely word in a context defined by other words\n","- training is unsupervised: no labeling of text is needed\n","\n","\n","The resulting vector of each word somehow denotes its meaning: **semantically similar words are represented with similar vectors**. Moreover, operations between vectors can be used to **find words semantically related** to each other.\n","\n","Word embedding models can be used to represent text in NLP tasks, including sentiment analysis\n","\n","The **gensim** library provides means to represent and build word embedding models"]},{"cell_type":"markdown","metadata":{"id":"twCE1oaJijh9"},"source":["## Training a Word2Vec model\n","\n","We have a set of 5,000 movie reviews without any labeling: we can't train a sentiment classifier on them but we can train a word embedding model\n","\n","We read the compressed text file `imdb-unsup-5k.txt.gz`, containing one review per line"]},{"cell_type":"code","metadata":{"id":"9OavdQnUijh-","executionInfo":{"status":"ok","timestamp":1623917404917,"user_tz":-120,"elapsed":1391,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"imdb-unsup-5k.txt.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-unsup-5k.txt.gz\")"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyBYGNk3ijiA","executionInfo":{"status":"ok","timestamp":1623917405570,"user_tz":-120,"elapsed":276,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["with gzip.open(\"imdb-unsup-5k.txt.gz\", \"rt\", encoding=\"utf8\") as f: # open gzip file containing the dataset\n","    we_train_set = [strip_tags(line.strip()) for line in f] # for each line (review) of the file, strip the string and map tags"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5GZooXWijiH"},"source":["We have to preprocess each review by splitting text into tokens\n","\n","gensim, the library used to train the word embedding model, provides a simple utility function for this\n","- alternatively any tokenization function can be used, e.g. `nltk.word_tokenize`"]},{"cell_type":"code","metadata":{"id":"PfutSnpbijiH","executionInfo":{"status":"ok","timestamp":1623917414884,"user_tz":-120,"elapsed":282,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from gensim.utils import simple_preprocess"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85unLKi1ijiI","executionInfo":{"status":"ok","timestamp":1623917418593,"user_tz":-120,"elapsed":1361,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"9ff3dab7-db36-4e28-dbff-bcbc5ce1a26a"},"source":["%%time\n","\n","we_train_tokens = [simple_preprocess(text) for text in we_train_set] # we_train_tokens will be a matrix where the first axis contains each review tokens array while the second one represents the review tokens\n","\n","# Wall time: time elapsed according to the computer's internal clock\n","# User-cpu time: the amount of time spent executing user-code \n","# Sys cpu time: the amount of time spent in the kernel due to the need of privileged operations (like IO to disk)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["CPU times: user 1.13 s, sys: 30.7 ms, total: 1.16 s\n","Wall time: 1.16 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gQK-0EreijiJ","executionInfo":{"status":"ok","timestamp":1623917421384,"user_tz":-120,"elapsed":295,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"a07f488e-f3a4-49cd-f7c0-6f0ed6401df8"},"source":["we_train_set[0][:82]"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'I admit, the great majority of films released before say 1933 are just not for me.'"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0s69SngijiK","executionInfo":{"status":"ok","timestamp":1623917430113,"user_tz":-120,"elapsed":256,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"64b64aec-8015-479f-cf2f-fc66ff3ecadb"},"source":["we_train_tokens[0][:8]"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['admit', 'the', 'great', 'majority', 'of', 'films', 'released', 'before']"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"ja20QoU3ijiM"},"source":["We can now use the token sequences to train the Word2Vec embedding model\n","\n","The most important parameter is the size of the word vectors we want to obtain\n","- in the original Word2Vec paper 300 is indicated as a good value\n","- here we use 50 as a tradeoff between accuracy and efficiency"]},{"cell_type":"code","metadata":{"id":"TutdIlQPijiM","executionInfo":{"status":"ok","timestamp":1623917433996,"user_tz":-120,"elapsed":271,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["wordvecs_size = 50"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RqPntI9ijiO"},"source":["Other relevant parameters are\n","- the _window size_, i.e. the number of words before and after any word to consider as its context\n","- the minimum appearances of a term to be included in the model\n","\n","We specify these options in the `Word2Vec` initializer, together with the set of token sequences used to train the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozAYVO4DijiO","executionInfo":{"status":"ok","timestamp":1623917494343,"user_tz":-120,"elapsed":8384,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"e1394eb3-e46c-43e5-f6f4-afdeaa6e1df9"},"source":["%%time\n","wv_model = gensim.models.Word2Vec(\n","    we_train_tokens,\n","    size=wordvecs_size,\n","    window=5,\n","    min_count=5\n",")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["CPU times: user 13.6 s, sys: 84.4 ms, total: 13.7 s\n","Wall time: 8.14 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uqu4z2HdijiQ"},"source":["Our Word2Vec model is now trained, we can get a reference to the word->vector mapping itself `wv` and drop the rest of the model object to free some memory"]},{"cell_type":"code","metadata":{"id":"NJ9oaU44ijiQ","executionInfo":{"status":"ok","timestamp":1623917494344,"user_tz":-120,"elapsed":28,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["wv = wv_model.wv\n","del wv_model"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-ZJts5QijiR"},"source":["## Exploring the word embedding model\n","\n","How many distinct terms are represented in the model?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cqNQUweijiR","executionInfo":{"status":"ok","timestamp":1623917519511,"user_tz":-120,"elapsed":265,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"d77d73dc-9fcc-45c2-9436-dd60e432b765"},"source":["len(wv.vocab)"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12070"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"8mMR884oijiT"},"source":["Which are these terms? `index2word` is an ordered list with more common terms coming first"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnGJ4SZpijiT","executionInfo":{"status":"ok","timestamp":1623917521647,"user_tz":-120,"elapsed":289,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"1a0cb3bb-c416-4d0e-9f1c-e4a16c4867c9"},"source":["wv.index2word[:10]"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'and', 'of', 'to', 'is', 'in', 'it', 'this', 'that', 'as']"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"UmCb-FNoijiU"},"source":["Let's see the vector of a word, e.g. \"excellent\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tte_Q-IjijiU","executionInfo":{"status":"ok","timestamp":1623917522563,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"c69332ec-b694-4c85-954e-07524c833e53"},"source":["wv.word_vec(\"excellent\")"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.283404  ,  0.03900452, -0.49420527,  0.72707194,  0.15900066,\n","       -0.79914004,  0.096211  , -0.01331059,  0.52336425,  0.10523325,\n","        0.5640176 , -0.83742225,  2.022225  , -1.203363  ,  1.4916005 ,\n","       -0.6019426 , -1.893869  , -0.5980516 , -0.31000522, -0.3418771 ,\n","       -0.41492155,  0.9883704 , -0.19136764,  0.3990537 ,  0.30418172,\n","       -0.29761457, -0.55882484,  0.03755228, -1.9207575 ,  1.5135777 ,\n","       -0.24472395,  0.19862314,  0.1313969 , -1.5651714 ,  1.8844779 ,\n","       -0.2493111 ,  0.07629514,  0.35472056,  3.2474585 ,  1.913897  ,\n","       -0.05061803,  1.1175715 , -0.8242053 , -0.5500437 ,  0.08416159,\n","        0.16943367,  0.33065522, -0.63872063, -1.4354265 , -1.235424  ],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"0bJN2kXWijiV"},"source":["We can also compute and get (L2) normalized word vectors (for each vector the sum of the respective components squares will always be up to 1), used to compute cosine similarity"]},{"cell_type":"code","metadata":{"id":"A6YbcAZtijiV","executionInfo":{"status":"ok","timestamp":1623917527156,"user_tz":-120,"elapsed":341,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["wv.init_sims()   # compute and cache normalized vectors (using L2-Normalization)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHTppXSMijiW","executionInfo":{"status":"ok","timestamp":1623917528962,"user_tz":-120,"elapsed":311,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"aa5729c2-e1c6-4bdf-b660-ff31d497b136"},"source":["# True indicates to normalize the vector\n","wv.word_vec(\"excellent\", True)"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.17902085,  0.00544071, -0.06893624,  0.1014186 ,  0.02217886,\n","       -0.11147131,  0.01342038, -0.00185668,  0.0730036 ,  0.01467889,\n","        0.0786743 , -0.11681126,  0.2820783 , -0.167856  ,  0.208062  ,\n","       -0.08396442, -0.26417407, -0.08342167, -0.04324234, -0.04768812,\n","       -0.05787703,  0.13786688, -0.0266937 ,  0.05566363,  0.04243003,\n","       -0.04151398, -0.07794996,  0.00523813, -0.26792473,  0.21112756,\n","       -0.03413632,  0.02770576,  0.01832843, -0.21832432,  0.26286408,\n","       -0.03477618,  0.01064234,  0.04947965,  0.45298502,  0.26696774,\n","       -0.00706066,  0.15588902, -0.11496764, -0.0767251 ,  0.01173962,\n","        0.02363415,  0.04612279, -0.08909456, -0.20022632, -0.17232817],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"unRqX91gijiX"},"source":["The word vector by itself doesn't give much information, but we can search for example which are the words with vectors most similar to this...\n","\n","Let's use the `cosine_similarities` function to compute similarity between this vector and all the other ones, stored in the `vector` array"]},{"cell_type":"code","metadata":{"id":"3xhp5jb5ijiX","executionInfo":{"status":"ok","timestamp":1623917531333,"user_tz":-120,"elapsed":296,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["similarities_to_excellent = wv.cosine_similarities(\n","    wv.word_vec(\"excellent\"),\n","    wv.vectors\n",")"],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eyh_jJsKijiZ"},"source":["We obtain an array of cosine similarity scores that has a component for each word represented in the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oh2tZ8_hbkDA","executionInfo":{"status":"ok","timestamp":1623917533323,"user_tz":-120,"elapsed":386,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"9099c1ab-18a7-4e73-84a3-cb7859a80afe"},"source":["similarities_to_excellent.shape"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12070,)"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2YQMfI4ijiZ","executionInfo":{"status":"ok","timestamp":1623917534538,"user_tz":-120,"elapsed":11,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"f397848b-a361-4b7f-a530-0a1aa741d647"},"source":["similarities_to_excellent[:5]"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.11660845,  0.23078342,  0.07585305, -0.14229453,  0.20517144],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"6fbKEv9Pijic"},"source":["Let's label them with the term they refer to and sort by descending values"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yX-uIkfLijic","executionInfo":{"status":"ok","timestamp":1623917537079,"user_tz":-120,"elapsed":259,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"96f80373-5764-41a7-8441-7205c6b4a03b"},"source":["pd.Series(\n","    similarities_to_excellent,\n","    wv.index2word\n",").sort_values(ascending=False).head(10) # sort the values by descending similarity score and take the first 10"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["excellent      1.000000\n","outstanding    0.896715\n","superb         0.895258\n","fine           0.887295\n","amazing        0.885376\n","fantastic      0.884631\n","brilliant      0.884021\n","terrific       0.856702\n","impressive     0.854944\n","wonderful      0.852005\n","dtype: float32"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"vwyeiX8Yijih"},"source":["In this way we found **other words** other than \"excellent\" with a **strong positive connotation**!\n","\n","For this the model provides a `most_similar` method, which also removes the reference word from the results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThbC8lr_ijih","executionInfo":{"status":"ok","timestamp":1623917545385,"user_tz":-120,"elapsed":284,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"a69934e8-6e81-426d-b3d0-7ab3770496a1"},"source":["wv.most_similar(\"excellent\")"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('outstanding', 0.8967145085334778),\n"," ('superb', 0.8952583074569702),\n"," ('fine', 0.8872944116592407),\n"," ('amazing', 0.8853756785392761),\n"," ('fantastic', 0.8846314549446106),\n"," ('brilliant', 0.8840208053588867),\n"," ('terrific', 0.8567023873329163),\n"," ('impressive', 0.8549444675445557),\n"," ('wonderful', 0.8520052433013916),\n"," ('directing', 0.8442342877388)]"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"eD6Rkhv-ijij"},"source":["We can similarly see what happens with a strongly negative word, e.g. \"terrible\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLCZfn4Tijij","executionInfo":{"status":"ok","timestamp":1623917548789,"user_tz":-120,"elapsed":254,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"81a78bcf-2fda-4a43-fb9a-d28ff3522bb0"},"source":["wv.most_similar(\"terrible\")"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('horrible', 0.9408929347991943),\n"," ('awful', 0.9047756195068359),\n"," ('ok', 0.8979083895683289),\n"," ('hilarious', 0.8972069025039673),\n"," ('totally', 0.8840056657791138),\n"," ('scary', 0.8839995265007019),\n"," ('ridiculous', 0.8771275281906128),\n"," ('boring', 0.8617339134216309),\n"," ('amazing', 0.85835862159729),\n"," ('absolutely', 0.8548043370246887)]"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"oRbp83_Mijil"},"source":["Other strongly negative words are found!"]},{"cell_type":"markdown","metadata":{"id":"VmXN-LSoijil"},"source":["Another powerful function of word embedding models is to find words with specific syntactic and semantic relationships using vector arithmetics\n","\n","Consider the relationship _\"man\" is to \"woman\" as \"actor\" is to X_ where the model has to find out that X = \"actress\"\n","\n","Word2Vec produces vectors in such a way that _\"man\" - \"woman\" = \"actor\" - X_, so we can find X as the term whose vector is closest to _\"actor\" + \"woman\" - \"man\"_\n","\n","Let's produce the vector representation of X..."]},{"cell_type":"code","metadata":{"id":"i2Qt-RL2ijim","executionInfo":{"status":"ok","timestamp":1623917553303,"user_tz":-120,"elapsed":253,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["composition = (wv.word_vec(\"actor\", True)\n","             + wv.word_vec(\"woman\", True)\n","             - wv.word_vec(\"man\", True))"],"execution_count":71,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AA_gUAY0ijio"},"source":["...and then find the words most similar to this composition"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrKeBry3ijio","executionInfo":{"status":"ok","timestamp":1623917558001,"user_tz":-120,"elapsed":337,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"39e7b646-8285-4254-f34f-a1ce8c1c7f1f"},"source":["pd.Series(\n","    wv.cosine_similarities(composition, wv.vectors),\n","    wv.index2word\n",").sort_values(ascending=False).head(10)"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["actor          0.918323\n","actress        0.912581\n","role           0.835966\n","talented       0.784195\n","performance    0.776335\n","played         0.775906\n","nathan         0.773076\n","chaney         0.765204\n","davis          0.763021\n","welles         0.751256\n","dtype: float32"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"KQgwZw9mijiq"},"source":["Also in this case we can use `most_similar`, distinguishing words with positive and negative weight"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUswR_k4ijiq","executionInfo":{"status":"ok","timestamp":1623917561464,"user_tz":-120,"elapsed":252,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"27486cd6-44ea-45df-932b-0b6f59d623d8"},"source":["wv.most_similar(\n","    positive=[\"actor\", \"woman\"],\n","    negative=[\"man\"]\n",")"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('actress', 0.9125813245773315),\n"," ('role', 0.8359656929969788),\n"," ('talented', 0.7841951847076416),\n"," ('performance', 0.7763352990150452),\n"," ('played', 0.7759054899215698),\n"," ('nathan', 0.7730758786201477),\n"," ('chaney', 0.7652037739753723),\n"," ('davis', 0.7630206346511841),\n"," ('welles', 0.7512555122375488),\n"," ('bruce', 0.7477737665176392)]"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"b6AiDV0Oijir"},"source":["According to randomness in the training process, the correct answer \"actress\" might be the most similar word or very close to it, but still the confidence of the model is limited"]},{"cell_type":"markdown","metadata":{"id":"OUhtaQjaijir"},"source":["We proceed our analysis on a pretrained GloVe (_Global Vectors_) word embedding model, whose training procedure is similar to Word2Vec\n","\n","We use a version trimmed down to the most common 100,000 terms of the 100d model trained on Wikipedia, available here: https://nlp.stanford.edu/projects/glove/\n","\n","Arrays with words and vectors are provided in the `glove.npz` file"]},{"cell_type":"code","metadata":{"id":"LtxBxAWlijir","executionInfo":{"status":"ok","timestamp":1623917570659,"user_tz":-120,"elapsed":1583,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"glove.npz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/glove.npz\")"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ro3dVXHVijis","executionInfo":{"status":"ok","timestamp":1623917570662,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["with np.load(\"glove.npz\") as f:\n","    glove_words = f[\"words\"]\n","    glove_vectors = f[\"vectors\"]"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gqHrkpgijiu"},"source":["We read the vector size from the loaded array"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zc8Ezgocijiu","executionInfo":{"status":"ok","timestamp":1623917572890,"user_tz":-120,"elapsed":240,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"9e62a76c-0f6c-4758-d0fa-bd8098200d68"},"source":["wordvecs_size = glove_vectors.shape[1]\n","wordvecs_size"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"vXSAB-Fiijiv"},"source":["We then create the word embedding model from the words"]},{"cell_type":"code","metadata":{"id":"810O0gzOijiv","executionInfo":{"status":"ok","timestamp":1623917575530,"user_tz":-120,"elapsed":604,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["wv = gensim.models.KeyedVectors(wordvecs_size)\n","wv[glove_words.tolist()] = glove_vectors\n","wv.init_sims()"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXM_8bZBijiw"},"source":["Searching on this model for the answer to _man : woman = actor : X_..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ts4n0Lzoijiw","executionInfo":{"status":"ok","timestamp":1623917577421,"user_tz":-120,"elapsed":302,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"1ad7cdf0-35d0-4a6f-d895-567fd8b15671"},"source":["wv.most_similar(\n","    positive=[\"actor\", \"woman\"],\n","    negative=[\"man\"]\n",")"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('actress', 0.9073609113693237),\n"," ('comedian', 0.6890829205513),\n"," ('actresses', 0.6826434135437012),\n"," ('screenwriter', 0.6554961204528809),\n"," ('starred', 0.6533135175704956),\n"," ('starring', 0.6514240503311157),\n"," ('actors', 0.6402771472930908),\n"," ('dancer', 0.6378583908081055),\n"," ('singer', 0.6346279382705688),\n"," ('filmmaker', 0.6279778480529785)]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"Zqk6qXhPijix"},"source":["...the correct answer \"actress\" is more dominant on the other ones"]},{"cell_type":"markdown","metadata":{"id":"VHOqPlRtijix"},"source":["Other examples with multiple pairs: finding the plural of a singular word..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmdt3u5Lijiy","executionInfo":{"status":"ok","timestamp":1623917580689,"user_tz":-120,"elapsed":484,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"ba92fd18-4c1d-42dc-cfd4-6b7a122c3881"},"source":["wv.most_similar(\n","    positive=[\"mouse\", \"dogs\", \"cats\"],\n","    negative=[         \"dog\",  \"cat\"]\n",")"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('mice', 0.710668683052063),\n"," ('rabbits', 0.681904673576355),\n"," ('rodents', 0.6771590709686279),\n"," ('rats', 0.6427716016769409),\n"," ('animals', 0.6243681907653809),\n"," ('monkeys', 0.6002902984619141),\n"," ('ferrets', 0.5910987854003906),\n"," ('mammals', 0.5888075828552246),\n"," ('foxes', 0.5750464200973511),\n"," ('raccoons', 0.5635854005813599)]"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"QEzzTgM6ijiz"},"source":["...and finding the capital of a State"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6xfzMegijiz","executionInfo":{"status":"ok","timestamp":1623917584972,"user_tz":-120,"elapsed":259,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"254e867e-e7cb-452d-a831-3fbf49ee38ce"},"source":["wv.most_similar(\n","    positive=[\"france\", \"rome\",  \"berlin\"],\n","    negative=[          \"italy\", \"germany\"]\n",")"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('paris', 0.7584174871444702),\n"," ('cairo', 0.6146870851516724),\n"," ('london', 0.5959091186523438),\n"," ('versailles', 0.5937519669532776),\n"," ('vienna', 0.5896108150482178),\n"," ('brussels', 0.5775601863861084),\n"," ('petersburg', 0.5704914331436157),\n"," ('palace', 0.5681281089782715),\n"," ('sorbonne', 0.5556104183197021),\n"," ('strasbourg', 0.555298388004303)]"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"dtlGXDtDiji0"},"source":["Another method provided by the model is `doesnt_match` finding the word which is the least related to the others in a given list"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"ggqHsg-Ciji0","executionInfo":{"status":"ok","timestamp":1623917600548,"user_tz":-120,"elapsed":325,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"5ea462d4-862c-417e-9bf5-9258683c3879"},"source":["wv.doesnt_match([\"cat\", \"mouse\", \"dog\", \"keyboard\", \"frog\"])"],"execution_count":83,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'keyboard'"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"markdown","metadata":{"id":"zbF34BR6iji1"},"source":["## Representing text with word embedding\n","\n","We now see how to leverage the word embedding model in a neural network for sentiment classification\n","\n","We start by tokenizing texts of training reviews"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fl71GbuEiji1","executionInfo":{"status":"ok","timestamp":1623917622075,"user_tz":-120,"elapsed":6025,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"1603a5b1-c0af-4900-d9cb-b3428c78b45b"},"source":["%%time\n","train_tokens = [gensim.utils.simple_preprocess(text) for text in train_set[\"text\"]]"],"execution_count":84,"outputs":[{"output_type":"stream","text":["CPU times: user 5.6 s, sys: 150 ms, total: 5.75 s\n","Wall time: 5.75 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dxQCdAA2iji2"},"source":["Let's see an example of tokenized review"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LeDkVtk2iji2","executionInfo":{"status":"ok","timestamp":1623917622077,"user_tz":-120,"elapsed":34,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"45c163dc-ffd5-42f5-e37e-bed4696adde7"},"source":["train_set[\"text\"][0][:34]"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Bromwell High is a cartoon comedy.'"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExehX2fviji3","executionInfo":{"status":"ok","timestamp":1623917622079,"user_tz":-120,"elapsed":29,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"9a735958-ebae-4fd5-bc2e-6c77e001888f"},"source":["train_tokens[0][:5]"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bromwell', 'high', 'is', 'cartoon', 'comedy']"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"ogIckRWKiji4"},"source":["We now convert these lists of text tokens into lists of indices of terms in the word embedding model, leaving out terms not present in the model"]},{"cell_type":"code","metadata":{"id":"w3iIvvERiji4","executionInfo":{"status":"ok","timestamp":1623917626389,"user_tz":-120,"elapsed":2094,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["train_indices = [\n","    [wv.vocab[word].index for word in text if word in wv.vocab] # for each token (word) in the review, that is present in our Word2Vec vocabulary, get its index in the Word2Vec vocabulary\n","    for text in train_tokens # i.e. for each tokenized review\n","]"],"execution_count":87,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRPPA1oeiji5"},"source":["For example the begin of the review above is now represented with..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIsZ8aQniji5","executionInfo":{"status":"ok","timestamp":1623917628857,"user_tz":-120,"elapsed":267,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"6266c98b-52e8-44ed-8d67-fe3f449dfb00"},"source":["train_indices[0][:5] # [0] is the index of the first review, [:5] is used to get the first five word indexes of the first review"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[152, 14, 7362, 2841, 20]"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"pxbZlUmZiji6"},"source":["...which translated back into words would be... (notice that the first term was removed because not in the embedding model)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OZ7A9BIiji6","executionInfo":{"status":"ok","timestamp":1623917630942,"user_tz":-120,"elapsed":247,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"badc4bad-150d-4596-99a8-c74c64e4c96c"},"source":["[wv.index2word[i] for i in train_indices[0][:5]]"],"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['high', 'is', 'cartoon', 'comedy', 'it']"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"pHMbszeJiji8"},"source":["Since we want to perform a review-level sentiment analysis, we have to find a way to represent each review using the respective word vectors.\n","As a first solution, we represent each review with the mean of normalized vectors of words contained in it: we obtain such vectors for all train reviews and stack them together in a matrix"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbiYYgapiji8","executionInfo":{"status":"ok","timestamp":1623917634247,"user_tz":-120,"elapsed":1247,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"4e6108a6-f67b-4bf9-debb-3645e3b8d168"},"source":["train_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in train_indices]) # i.e. for each review indices, get the relative word2vec vectors, compute their means and stack the resulting vectors in a matrix.\n","\n","# This way we now have a matrix that has a row per training set review and as many columns as the number of word vector features (100)\n","train_we_repr.shape"],"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 100)"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"itLB8GMyiji9"},"source":["We then create a MLP network with one hidden layer accepting such vectors in input"]},{"cell_type":"code","metadata":{"id":"nImeccYXiji9","executionInfo":{"status":"ok","timestamp":1623917643334,"user_tz":-120,"elapsed":284,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model = Sequential([\n","    Dense(128, activation=\"sigmoid\", input_dim=wordvecs_size),\n","    Dense(2, activation=\"softmax\")\n","])\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mXR0j5rJiji-"},"source":["As the input size of the network is much lower, so it is the number of parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7LO5JNKiji-","executionInfo":{"status":"ok","timestamp":1623917645344,"user_tz":-120,"elapsed":284,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"7ea45f33-cd27-44d7-e3a3-a892cb263e96"},"source":["model.summary()"],"execution_count":92,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_11 (Dense)             (None, 128)               12928     \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 2)                 258       \n","=================================================================\n","Total params: 13,186\n","Trainable params: 13,186\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zerzdpkFiji_"},"source":["Training is much faster than before, so we can increment the epochs and reduce the batch size, thus making more SGD steps in each epoch"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVyXA1WkijjA","executionInfo":{"status":"ok","timestamp":1623917676136,"user_tz":-120,"elapsed":28451,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"a9105075-6a22-4f44-d313-592abd2d2474"},"source":["model.fit(train_we_repr, train_target, batch_size=20, epochs=10)"],"execution_count":93,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","25000/25000 [==============================] - 3s 115us/step - loss: 0.6527 - accuracy: 0.6312\n","Epoch 2/10\n","25000/25000 [==============================] - 3s 113us/step - loss: 0.5700 - accuracy: 0.7242\n","Epoch 3/10\n","25000/25000 [==============================] - 3s 111us/step - loss: 0.5199 - accuracy: 0.7573\n","Epoch 4/10\n","25000/25000 [==============================] - 3s 110us/step - loss: 0.4931 - accuracy: 0.7732\n","Epoch 5/10\n","25000/25000 [==============================] - 3s 111us/step - loss: 0.4794 - accuracy: 0.7791\n","Epoch 6/10\n","25000/25000 [==============================] - 3s 111us/step - loss: 0.4699 - accuracy: 0.7843\n","Epoch 7/10\n","25000/25000 [==============================] - 3s 111us/step - loss: 0.4648 - accuracy: 0.7833\n","Epoch 8/10\n","25000/25000 [==============================] - 3s 110us/step - loss: 0.4602 - accuracy: 0.7878\n","Epoch 9/10\n","25000/25000 [==============================] - 3s 111us/step - loss: 0.4592 - accuracy: 0.7895\n","Epoch 10/10\n","25000/25000 [==============================] - 3s 112us/step - loss: 0.4558 - accuracy: 0.7902\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa05f348dd0>"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"markdown","metadata":{"id":"NukXqBTwijjB"},"source":["Let's preprocess test reviews as we did for training ones, thus extracting tokens and converting them to indices..."]},{"cell_type":"code","metadata":{"id":"yBNZ24koijjB","executionInfo":{"status":"ok","timestamp":1623917685343,"user_tz":-120,"elapsed":7609,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["test_tokens = [gensim.utils.simple_preprocess(text) for text in test_set[\"text\"]]\n","test_indices = [\n","    [wv.vocab[word].index for word in text if word in wv.vocab]\n","    for text in test_tokens\n","]"],"execution_count":94,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75n6Yyz8ijjC"},"source":["...and obtaining means of word vectors for each review"]},{"cell_type":"code","metadata":{"id":"M_hMmd8aijjC","executionInfo":{"status":"ok","timestamp":1623917705299,"user_tz":-120,"elapsed":1436,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["test_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in test_indices])"],"execution_count":96,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LPWB-1nEijjF"},"source":["We can now evaluate the network on the test reviews"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjiIP9UpijjF","executionInfo":{"status":"ok","timestamp":1623917706546,"user_tz":-120,"elapsed":1250,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"e7bc4800-1595-4e2b-d693-88b2bfe59a1a"},"source":["model.evaluate(test_we_repr, test_target)"],"execution_count":97,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 1s 44us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.4674276295280457, 0.7822800278663635]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"sapvLiNFijjH"},"source":["The accuracy is not as good as before: with this representation we lose identity of the words in the documents other than their order"]},{"cell_type":"markdown","metadata":{"id":"q67ydteQijjI"},"source":["## Recurrent neural networks\n","\n","MLPs are _feed-forward_ networks: their output at any time is only dependent from their input at the same time\n","\n","On the other side, if we somehow introduce **memory** inside a network, we can make its output dependent from current as well as past inputs, thus we can process **sequential data**\n","\n","_Recurrent_ neural networks include **cyclic connections** between nodes, making the output dependent from the state of the network at previous time steps and thus from previous inputs"]},{"cell_type":"markdown","metadata":{"id":"EtGrQj7AijjI"},"source":["### Sequential data\n","\n","While an input example for a MLP must be represented with a vector of size S, an example for a recurrent NN is represented with a **sequence of vectors**, fed to the network in T subsequent time steps (T is equal for all examples)\n","\n","Thus N input samples with input size S are no longer represented with a N×S array, but with a N×T×S array\n","\n","Leveraging the word embedding model, we represent each review with the **sequence of word vectors** for the terms contained in it\n","- in this way, we consider both the identity of words (the vectors) and their order!"]},{"cell_type":"markdown","metadata":{"id":"t9ig7jcbijjJ"},"source":["We start from the sequences of word indices `*_indices` (train_indices, test_indices) extracted above\n","\n","We need to make all sequences of the same length (the T term above): we set a desired sequence size T, then we trim longer sequences to that size (taking the final T elements) and pad shorter sequences with null values: Keras' `pad_sequences` function does this\n","- larger T values would make training much slower"]},{"cell_type":"code","metadata":{"id":"xrJ9m443ijjK","executionInfo":{"status":"ok","timestamp":1623917719483,"user_tz":-120,"elapsed":719,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from keras.preprocessing.sequence import pad_sequences\n","max_words = 200\n","train_seq = pad_sequences(train_indices, max_words)"],"execution_count":98,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzShlwn7ijjL","executionInfo":{"status":"ok","timestamp":1623917719819,"user_tz":-120,"elapsed":7,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"6d49d167-6dba-46bb-8d19-863e0c797b1b"},"source":["train_seq"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[    0,     0,     0, ...,    12,    20, 75360],\n","       [    0,     0,     0, ...,    30,   541,  3442],\n","       [    0,     0,     0, ...,   219,   191,   219],\n","       ...,\n","       [    0,     0,     0, ..., 29080,  1075,    48],\n","       [  460,     4,    30, ...,   227,    30,  4254],\n","       [    0,     0,     0, ...,  1666,    13, 13664]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"markdown","metadata":{"id":"sTgjsU67ijjN"},"source":["The size of the matrix is the number of samples times the sequence length, i.e. N×T"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eso0uIZoijjO","executionInfo":{"status":"ok","timestamp":1623917722820,"user_tz":-120,"elapsed":421,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"d43955e4-82f8-4b4c-ecc4-84654df8a5dc"},"source":["train_seq.shape"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 200)"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"T9sOFUNDijjP"},"source":["### Building the network\n","\n","Let's now create a neural network which gets such sequences as input"]},{"cell_type":"code","metadata":{"id":"dCVlPbB9ijjP","executionInfo":{"status":"ok","timestamp":1623917724789,"user_tz":-120,"elapsed":5,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model = Sequential()"],"execution_count":101,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EuPG_WpcijjQ"},"source":["We first insert an `Embedding` layer, which translates each received value into the word vector from the embedding model\n","\n","We need to specify the size of input and output and the word vectors to be used, taking them from the model; we also specify `trainable=False` to \"freeze\" our pretrained word vectors and exclude them from training"]},{"cell_type":"code","metadata":{"id":"n0_fyctsijjQ","executionInfo":{"status":"ok","timestamp":1623917727152,"user_tz":-120,"elapsed":779,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from keras.layers import Embedding\n","model.add(Embedding(\n","    input_dim=len(wv.vocab),    # number of distinct vocabulary terms in Word2Vec model\n","    output_dim=wordvecs_size,   # size of word vectors (S)\n","    input_length=max_words,     # length of sequences (T)\n","    weights=[wv.vectors],       # pretrained Word2Vec vectors\n","    trainable=False\n","))"],"execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iusgIq9ijjR"},"source":["The output of this layer is a N×T×S tensor, we feed it to a recurrent layer which receives S-sized vectors for T time steps\n","\n","_Gated Recurrent Units_ (GRU) are a simplified version of _Long Short-Term Memory_ (LSTM) units, which can potentially hold information in memory across many time steps; we use here a layer of 128 GRU cells\n","\n","_Dropout_ randomly drops (sets to zero) a given ratio of input values at each time step: it is a technique to prevent model overfitting"]},{"cell_type":"code","metadata":{"id":"dE2umoGqijjS","executionInfo":{"status":"ok","timestamp":1623917729284,"user_tz":-120,"elapsed":404,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["from keras.layers import GRU\n","model.add(GRU(128, dropout=0.2))"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fz6cZX6tijjT"},"source":["While producing 128 output values at each time step, the GRU layer by default only returns the outputs at the final steps, i.e. when the whole input sequence has been fed to the network, thus the output size of this layer is N×128 (the time dimension collapses)\n","\n","We can now finalize the network with the output layer, which receives the output of the GRU layer"]},{"cell_type":"code","metadata":{"id":"oQQXSfhLijjT","executionInfo":{"status":"ok","timestamp":1623917730632,"user_tz":-120,"elapsed":413,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["model.add(Dense(2, activation=\"softmax\"))"],"execution_count":104,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQz40ZwHijjU"},"source":["The model summary gives a recap of shapes of data across network layers other than parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8XBIRFIijjU","executionInfo":{"status":"ok","timestamp":1623917732256,"user_tz":-120,"elapsed":291,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"bcf7e932-b464-4b76-fdd9-a53e2e028782"},"source":["model.summary()"],"execution_count":105,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 200, 100)          10000000  \n","_________________________________________________________________\n","gru_1 (GRU)                  (None, 128)               87936     \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 2)                 258       \n","=================================================================\n","Total params: 10,088,194\n","Trainable params: 88,194\n","Non-trainable params: 10,000,000\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vIP-bm4SijjV"},"source":["We can now compile the network and train it on the padded sequences of word indices\n","- training of RNNs is quite slow, we again limit training to 3 epochs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpWVXKdoijjW","executionInfo":{"status":"ok","timestamp":1623917797182,"user_tz":-120,"elapsed":63445,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"286e59a0-78c6-4ab5-c7ff-9d003e8c58bf"},"source":["model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","model.fit(train_seq, train_target, batch_size=200, epochs=3)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","25000/25000 [==============================] - 21s 847us/step - loss: 0.6032 - accuracy: 0.6605\n","Epoch 2/3\n","25000/25000 [==============================] - 21s 831us/step - loss: 0.4511 - accuracy: 0.7898\n","Epoch 3/3\n","25000/25000 [==============================] - 21s 831us/step - loss: 0.3953 - accuracy: 0.8234\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa04992fed0>"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"markdown","metadata":{"id":"jgwxQH-bijjX"},"source":["Let's now obtain the padded sequences also for the test reviews..."]},{"cell_type":"code","metadata":{"id":"qo6fNOX1ijjY","executionInfo":{"status":"ok","timestamp":1623917797660,"user_tz":-120,"elapsed":242,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["test_seq = pad_sequences(test_indices, max_words)"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g20kvlPHijjZ"},"source":["...and use them to evaluate the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5oOOjw0KijjZ","executionInfo":{"status":"ok","timestamp":1623917852844,"user_tz":-120,"elapsed":55186,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"c8dde1eb-db61-4c04-df01-c1b6c1cb1638"},"source":["model.evaluate(test_seq, test_target)"],"execution_count":108,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 55s 2ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.3491961818408966, 0.8466399908065796]"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"markdown","metadata":{"id":"F3Xvt97jZAM_"},"source":["We have got an higher accuracy than the previous solution, thanks to the reviews representation as word sequences and the memory capability of the GRU network"]},{"cell_type":"markdown","metadata":{"id":"_O5CvEQVijja"},"source":["## Cross domain classification\n","\n","We trained our network on reviews of movies and tested its ability to classify sentiment in reviews of movies\n","\n","Can we successfully apply our model to reviews pertaining to a different domain?\n","\n","The `yelp-test-10k.csv.gz` file contains 10,000 labeled user reviews about restaurants extracted from Yelp"]},{"cell_type":"code","metadata":{"id":"ruAqMWKuijja","executionInfo":{"status":"ok","timestamp":1623917881549,"user_tz":-120,"elapsed":259,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"yelp-test-10k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-test-10k.csv.gz\")"],"execution_count":112,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9WF8IjCijjb","executionInfo":{"status":"ok","timestamp":1623917882607,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["xdom_set = pd.read_csv(\"yelp-test-10k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"],"execution_count":113,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"VVAq0BiMijjc","executionInfo":{"status":"ok","timestamp":1623917883328,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"8167df47-4d1c-4cce-ce32-5b8c24887b9f"},"source":["xdom_set.head(5)"],"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfec...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neg</td>\n","      <td>U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neg</td>\n","      <td>Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's sty...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                                                                                 text\n","0   pos   My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfec...\n","1   neg   U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a...\n","2   pos   I have no idea why some people give bad reviews about this place. It goes to show you, you can ...\n","3   neg   Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's sty...\n","4   pos   Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of..."]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pH0eU8nijjf","executionInfo":{"status":"ok","timestamp":1623917886057,"user_tz":-120,"elapsed":449,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"ed36c99e-e460-4b82-fbfc-7b364d3837dc"},"source":["xdom_set[\"label\"].value_counts()"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neg    5000\n","pos    5000\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"bn2HaiYOijji"},"source":["We apply the same preprocessing steps we applied above"]},{"cell_type":"code","metadata":{"id":"dbvFQXrHijji","executionInfo":{"status":"ok","timestamp":1623917890435,"user_tz":-120,"elapsed":2640,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["xdom_set[\"text\"] = xdom_set[\"text\"].apply(strip_tags)\n","xdom_tokens = [gensim.utils.simple_preprocess(text) for text in xdom_set[\"text\"]]\n","xdom_indices = [\n","    [wv.vocab[word].index for word in text if word in wv.vocab]\n","    for text in xdom_tokens\n","]\n","xdom_seq = pad_sequences(xdom_indices, max_words)\n","xdom_target = make_target(xdom_set[\"label\"])"],"execution_count":116,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSNJx7E-ijjj","executionInfo":{"status":"ok","timestamp":1623917916115,"user_tz":-120,"elapsed":22273,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"967efac2-7ba6-479c-c069-92ab771ceffc"},"source":["model.evaluate(xdom_seq, xdom_target)"],"execution_count":117,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 22s 2ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.38018616147041323, 0.8310999870300293]"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"markdown","metadata":{"id":"co0Aap_qijjj"},"source":["The network is fairly accurate, although it was trained on reviews of a different domain\n","\n","Can we further improve this?"]},{"cell_type":"markdown","metadata":{"id":"m49UfOgBijjk"},"source":["## Fine tuning the network\n","\n","In the `yelp-train-2k.csv.gz` we have a set of 2,000 labeled Yelp reviews which can be used for training\n","\n","We would like to make use of these in-domain reviews, without throwing away the model trained on the richer set of cross-domain reviews\n","\n","We can \"tune\" the trained model with an additional training run on the new set of reviews, thus making it more oriented to the new domain and still using knowledge from the other\n","\n","Let's load and view a summary of the file..."]},{"cell_type":"code","metadata":{"id":"nxlsR6mBijjk","executionInfo":{"status":"ok","timestamp":1623917921987,"user_tz":-120,"elapsed":812,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["download(\"yelp-train-2k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-train-2k.csv.gz\")"],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxpazN57ijjk","executionInfo":{"status":"ok","timestamp":1623917922564,"user_tz":-120,"elapsed":583,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["tune_set = pd.read_csv(\"yelp-train-2k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"],"execution_count":119,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"MCs5AdGVijjl","executionInfo":{"status":"ok","timestamp":1623917923123,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"e83ddb2a-6c1f-493f-f89f-b09c2994af01"},"source":["tune_set.head(5)"],"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Great local yoga studio. Had flexible hours like early morning and late night to fit any schedu...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neg</td>\n","      <td>I've been craving a good roast beef sandwich for a few days now, and finally had the chance to ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Super tasty, love the cozy atmosphere, excellent and friendly service!  The naan was a bit thin...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neg</td>\n","      <td>After waiting 4 days to get an appointment, Flores was a no show and didn't even bother to call.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>I have had my kitty Miller for 8 years. She has never been to any other vet.  I like this place...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                                                                                 text\n","0   pos   Great local yoga studio. Had flexible hours like early morning and late night to fit any schedu...\n","1   neg   I've been craving a good roast beef sandwich for a few days now, and finally had the chance to ...\n","2   pos   Super tasty, love the cozy atmosphere, excellent and friendly service!  The naan was a bit thin...\n","3   neg     After waiting 4 days to get an appointment, Flores was a no show and didn't even bother to call.\n","4   pos   I have had my kitty Miller for 8 years. She has never been to any other vet.  I like this place..."]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8DJfLp6ijjm","executionInfo":{"status":"ok","timestamp":1623917925402,"user_tz":-120,"elapsed":277,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"460ca180-9f01-4032-8437-6f89514700a5"},"source":["tune_set[\"label\"].value_counts()"],"execution_count":121,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pos    1000\n","neg    1000\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"markdown","metadata":{"id":"UzUliT1fijjn"},"source":["...and apply the usual preprocessing steps"]},{"cell_type":"code","metadata":{"id":"Sljc7xaKijjn","executionInfo":{"status":"ok","timestamp":1623917927671,"user_tz":-120,"elapsed":437,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}}},"source":["tune_set[\"text\"] = tune_set[\"text\"].apply(strip_tags)\n","tune_tokens = [gensim.utils.simple_preprocess(text) for text in tune_set[\"text\"]]\n","tune_indices = [\n","    [wv.vocab[word].index for word in text if word in wv.vocab]\n","    for text in tune_tokens\n","]\n","tune_seq = pad_sequences(tune_indices, max_words)\n","tune_target = make_target(tune_set[\"label\"])"],"execution_count":122,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_tp7lZjijjo"},"source":["We now repeat the model training process on this set of reviews: the process is very fast due to the limited size of the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0UcqKYrijjo","executionInfo":{"status":"ok","timestamp":1623917937534,"user_tz":-120,"elapsed":8503,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"43530b91-fe02-4609-a1a6-17e3e9a2a159"},"source":["model.fit(tune_seq, tune_target, epochs=5, batch_size=200)"],"execution_count":123,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","2000/2000 [==============================] - 2s 845us/step - loss: 0.3890 - accuracy: 0.8220\n","Epoch 2/5\n","2000/2000 [==============================] - 2s 829us/step - loss: 0.3174 - accuracy: 0.8600\n","Epoch 3/5\n","2000/2000 [==============================] - 2s 824us/step - loss: 0.2936 - accuracy: 0.8780\n","Epoch 4/5\n","2000/2000 [==============================] - 2s 825us/step - loss: 0.2671 - accuracy: 0.8920\n","Epoch 5/5\n","2000/2000 [==============================] - 2s 845us/step - loss: 0.2588 - accuracy: 0.8910\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa03f423d10>"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"markdown","metadata":{"id":"pUtScIhQijjp"},"source":["Let's now repeat the evaluation on the Yelp test set loaded before"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKPuFAkRijjp","executionInfo":{"status":"ok","timestamp":1623917960827,"user_tz":-120,"elapsed":22114,"user":{"displayName":"Nicola Piscaglia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN3sigr_aQt9oU7hyti7jsOitPy9mfYmqQjkKZ=s64","userId":"06169751944750633598"}},"outputId":"17c54846-c4b3-4efd-b569-a4a175037dfc"},"source":["model.evaluate(xdom_seq, xdom_target)"],"execution_count":124,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 22s 2ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.2311825002551079, 0.9077000021934509]"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"markdown","metadata":{"id":"a3MGAFWQijjq"},"source":["We successfully boosted the model accuracy, combining even limited knowledge of the target domain with large knowledge extracted from a different domain"]}]}